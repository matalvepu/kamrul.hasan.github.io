<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Resume - Start Bootstrap Theme</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.13.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <!-- <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" /> -->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Hasan Kamrul</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="" /></span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger"  href="#experience">Experience</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#publication">Publications</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#reserach">Research</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#resources">Resources</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#contact">Contact</a></li>
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <!-- <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Kamrul
                        <span class="text-primary">Hasan</span>
                    </h1>
                    <div class="subheading mb-5">
                       PhD Candidate @ University of Rochester
                    </div>
                        <p class="lead mb-5 " style="color: black; font-size: 14px;"> I am a PhD candidate at the Univcersity of Rochester, USA. I received my MS degree in Computer Science from the University of Rochester in 2018 and B.Sc. in Computer Science and Engineering from Bangladesh University of Engineering and Technology in 2014.
                        </br> 
                        My research focused on theoretical and empirical aspects of <b>multimodal machine learning</b>. It is concerned with joint modeling ofmultiple modalities (language, acoustic & visual) to understand human language as it happens face-to-face communications. My supervisor is <a href="https://www.aclweb.org/anthology/D19-1211/" style="color:blue; text-decoration: underline;">Ehsan Hoque</a>. 
                        </br></br> 
                        <a href="https://hoques.com/" style="color:blue; text-decoration: underline;">resume</a> 
                        </br></br> 
                        </p>
                    </div> 
                    </br></br> </br></br> </br></br> 
                    <div class="social-icons">
                        <a class="social-icon" href="https://www.linkedin.com/in/md-kamrul-hasan-06a02845/"><i class="fab fa-linkedin-in"></i></a>
                        <a class="social-icon" href="https://github.com/matalvepu"><i class="fab fa-github"></i></a>
                        <a class="social-icon" href="#"><i class="fab fa-twitter"></i></a>
                        <a class="social-icon" href="#"><i class="fab fa-google"></i></a>
                    </div>
                </div>
            </section> -->

            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Kamrul
                        <span class="text-primary">Hasan</span>
                    </h1>
                    <div class="subheading mb-5">
                       PhD Candidate @ University of Rochester
                    </div>
                        <p class="lead mb-5 " style="color: black; font-size: 16px;"> 
                            I am a PhD candidate at the Univcersity of Rochester, USA. I received my MS degree in Computer Science from the University of Rochester in 2018 and B.Sc. in Computer Science and Engineering from Bangladesh University of Engineering and Technology in 2014.
                            </br> </br> 
                            My research focused on theoretical and empirical aspects of <b>multimodal machine learning</b>. It is concerned with joint modeling of multiple modalities (language, acoustic & visual) to understand human language as it happens face-to-face communications. My research work spans the following areas: multimodal representation learning, humor, sentiment, argument & credibility understanding in video utterance. My supervisor is <a href="https://hoques.com/" style="color:blue; text-decoration: underline;">Ehsan Hoque</a>. 
                            </br></br> 
                            <a href="https://drive.google.com/file/d/1zukVWWDGzetZwWczInAJDqho1Rt8JkIl/view?usp=sharing" style="color:blue; text-decoration: underline; font-size: 20px;">Resume</a> 
                            </br> 
                        </p>
                    
                    <div class="social-icons">
                        <a class="social-icon" href="https://www.linkedin.com/in/md-kamrul-hasan-06a02845/" style="width:50px;height: 50px;"><i class="fab fa-linkedin-in"></i></a>
                        <a class="social-icon" style="width:50px;height: 50px;" href="https://scholar.google.com/citations?user=zLGFlIgAAAAJ&hl=en"><i class="ai ai-google-scholar ai-1x"></i></a>
                        <a class="social-icon" style="width:50px;height: 50px;" href="https://github.com/matalvepu"><i class="fab fa-github"></i></a>
                        <!-- <a class="social-icon" style="width:50px;height: 50px;" href="https://scholar.google.com/citations?user=zLGFlIgAAAAJ&hl=en"><i class="fab fa-google"></i></a> -->
                        

                       <!--  <a class="social-icon" style="width:50px;height: 50px;" href="https://scholar.google.com/citations?user=zLGFlIgAAAAJ&hl=en"><i class="ai ai-google-scholar"></i></a> -->

                    </div>
                    </br></br>
                    <heading style="color: maroon; font-weight: bold;font-size: 24px;">News</heading>
                      <!-- <div class="container"> -->
                      <div id="content-news" class="lead mb-5 " style="color: black; font-size: 16px;" >
                      <!-- <div class="scroll">  -->

                      <table border=0 ><tbody>
                     
                <!--           <tr>
                              <td><p style="color:darkblue; display:inline">June '21 &nbsp</p></td>
                              <td>We are the winner of <a href="https://eval.ai/web/challenges/challenge-page/906/overview">TextCaps Challenge 2021</a> and <a href="https://referit3d.github.io/benchmarks.html">ReferIt3D Challenge 2021</a>. Welcome to check the related <a href="https://arxiv.org/pdf/2012.04638.pdf">TAP</a> and <a href="https://arxiv.org/pdf/2105.11450.pdf">SAT</a> papers.</td>
                          </tr>
                          <tr>
                              <td><p style="color:darkblue; display:inline">June '21 &nbsp</p></td>
                              <td>I defensed my Ph.D. dissertation and will join Microsoft as a Researcher.</td>
                          </tr>
                          <tr>
                              <td><p style="color:darkblue; display:inline">May '21 &nbsp</p></td>
                              <td>I am selected as one of <a href="http://cvpr2021.thecvf.com/node/184">Outstanding Reviewer</a> for CVPR 2021.</td>
                          </tr>
                          <tr>
                              <td><p style="color:darkblue; display:inline">Feb '21 &nbsp</p></td>
                              <td>Two papers accepted by CVPR 2021 (The <a href="https://github.com/microsoft/TAP">TAP</a> paper was selected as Oral).</td>
                          </tr>
                 -->                
                                <ps>
                                    <li> [2021] &nbsp EMNLP: one paper got accepted - multimodal argument analysis.</li>
                                    <li> [2021] &nbsp One paper got accepted in IEEE Transactions on Affective Computing.</li>
                                    <li> [2020] &nbsp AAAI: one paper got accepted - multimodal humor understanding.</li>
                                    <li> [2020] &nbsp ACL: one paper got accepetd - Multimodal Finetuning of Advanced Transformer Models.</li>
                                    <li> [2020] &nbsp Passed thesis proposal "Multimodal Represenation Learning for Human Behavior Understanding". </li>
                                    <li> [2019] &nbsp EMNLP: one paper got accepted - UR-FUNNY dataset.</li>
                                </ps>


                      </tbody></table>
                      </div>
                </div>
            </section>

            <hr class="m-0" />
            <!-- Experience-->
            <section class="resume-section" id="experience">
                <div class="resume-section-content">
                    <h2 class="mb-5" style="margin-bottom: 1.5rem !important;">Experience</h2>
                    <!--<div class="d-flex flex-column flex-md-row justify-content-between mb-5" style="margin-bottom: 1rem !important;">
                        <div class="flex-grow-1">
                     -->        <!--<h3 class="mb-2">Research Intern</h3>-->
                    <!--         <heading style="font-size: 28px;">Research Intern</heading>
                     -->        <!--<div class="subheading mb-3">Comcast AI</div>-->
                            <!-- <div style="color: maroon;font-size: 20px;">Comcast AI</div>
                            <p>Developed unsupervised text segmentation algorithm. </p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">July 2020 - Sept 2020</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5" style="margin-bottom: 1rem !important;">
                        <div class="flex-grow-1"> -->
                            <!-- <h3 class="mb-2">Applied Scientist Intern</h3>
                            <div class="subheading mb-3">Amazon</div> -->
                            <!-- <heading style="font-size: 28px;">Applied Scientist Intern</heading>
                            <div style="color: maroon;font-size: 20px;">Amazon</div>
                            <p>Developed algortihm for multimodal emotion recognition.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">May 2019 - Aug 2019</span></div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between" style="margin-bottom: 1rem !important;">
                        <div class="flex-grow-1">
                            <heading style="font-size: 28px;">Software Engineer</heading>
                            <div style="color: maroon;font-size: 20px;">Therap Services LLC</div> -->
                            <!-- <h3 class="mb-2">Software Engineer</h3>
                            <div class="subheading mb-3">Therap Services LLC</div> -->
                            <!-- <p>.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">Aug 2014 - May 2015</span></div>
                    </div> -->
                    <table width="100%" align="center" border="0" cellpadding="20" style="font-size:14px">
                        <tbody><tr>
                            <td width="25%">
                                <img src="assets/img/comcast.jpg"  width="150" height="80">
                            </td>
                            
                            <td width="75%" valign="center">
                            <p>
                                <papertitle >Research Intern </papertitle><br>
                                <papertitle style="color:#17a2b8">Comcast AI, Washington DC </papertitle><br>
                                </a>
                                Jul - Sep 2020. Advisor: <a style=" color: blue; text-decoration: underline;" href="https://www.linkedin.com/in/mahmuducr/"> Mahmudul Hasan, 
                                </a><br>
                                Project: unsupervised text segmentation.
                            </p>
                            </td>
                        </tr>

                        
                        
                        <tr>
                            <td width="25%">
                                <img src="assets/img/amazon1.jpg"  width="150" height="80">
                            </td>
                            
                            <td width="75%" valign="center">
                            <p>
                                <papertitle >Applied Scientist Intern </papertitle><br>
                                <papertitle style="color:#17a2b8">Amazon, Sunnyvale, CA</papertitle><br>
                                </a>
                                May - Aug 2019. Advisor: <a style=" color: blue; text-decoration: underline;" href="https://www.linkedin.com/in/yelinkim/"> Yelin Kim, 
                                </a><br>
                                Project: multimodal emotion recognition from noisy data.
                            </p>
                            </td>
                        </tr>
                    
                        <tr>
                            <td width="25%">
                                <img src="assets/img/therap.jpeg"  width="150" height="90">
                            </td>
                            
                            <td width="75%" valign="center">
                            <p>
                                <papertitle>Software Engineer </papertitle><br>
                                <papertitle style="color:#17a2b8">Therap (BD) Ltd, Bangladesh </papertitle><br>
                                </a>
                                Aug 2014 - May 2015. 
                                </a><br>
                                Project: Worked with Java and J2EE technologies. Desinged web solutions based on Spring framework and Hibernate ORM tool.
                            </p>
                            </td>
                        </tr>
                        
                    </table>                    

                </div>
            </section>
            <hr class="m-0" />
            <!-- Education-->
            <section class="resume-section" id="publication" style="font-size:14px">
                <div class="resume-section-content">
                    <h2 class="mb-5">Publications</h2>
                    <div class="subheading mb-3">Highlighted Papers</div>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

                        <tbody><tr>
                        <!-- <tbody><tr bgcolor="#ffffd0"> -->
                        <td width="25%">
                            <a href="assets/img/dbates.png">
                            <img src="assets/img/dbates.png" width="200px" height="150px">
                        </td>
                        <td valign="top" width="75%">
                            <p>
                            <papertitle><a href="https://aclanthology.org/2021.emnlp-main.515/">Hitting your MARQ: Multimodal ARgument Quality Assessment in Long Debate Video</a></papertitle><br>
                            <strong>Md Kamrul Hasan</strong>, James Spann, Masum Hasan, Md Saiful Islam, Kurtis Haut, Rada Mihalcea, and Ehsan Hoque <br>
                            <b>EMNLP 2021</b> <br>
                            [<a href="https://aclanthology.org/2021.emnlp-main.515/">PDF</a>]<br>
                            <!-- [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/17534">PDF</a>]
                            [<a href="https://github.com/matalvepu/HKT">Code</a>]<br> -->
                            <!-- [<a href="./bibtex/yang2021empirical.txt">Bibtex</a>] -->
                            <!-- [<a href="https://referit3d.github.io/benchmarks.html">Benchmarks</a>] -->
                            <grey> Present the first comprehensive study on multimodal argument quality assessment. 
                        </td>
                    </tr>

                    <tr>
                        <!-- <tbody><tr bgcolor="#ffffd0"> -->
                        <td width="25%">
                            <a href="assets/img/hkt2.png">
                            <img src="assets/img/hkt2.png" width="230px" height="150px">
                        </td>
                        <td valign="top" width="75%">
                            <p>
                            <papertitle><a href="https://ojs.aaai.org/index.php/AAAI/article/view/17534">Humor Knowledge Enriched Transformer for Understanding Multimodal Humor</a></papertitle><br>
                            <strong>Md Kamrul Hasan</strong>, Sangwu Lee, Wasifur Rahman, Amir Zadeh, Rada Mihalcea, Louis-Philippe Morency, and Ehsan Hoque <br>
                            <b>AAAI 2021</b> <br>
                            [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/17534">PDF</a>]
                            [<a href="https://github.com/matalvepu/HKT">Code</a>]<br>
                            <!-- [<a href="./bibtex/yang2021empirical.txt">Bibtex</a>] -->
                            <!-- [<a href="https://referit3d.github.io/benchmarks.html">Benchmarks</a>] -->
                            <grey> Can machine recognise humorous punchline given the context story and all three modalities? 
                        </td>
                    </tr>

                    <tr>
                        <!-- <tbody><tr bgcolor="#ffffd0"> -->
                        <td width="25%">
                            <a href="assets/img/mbert1.png">
                            <img src="assets/img/mbert1.png" width="180px" height="150px">
                        </td>
                        <td valign="top" width="75%">
                            <p>
                            <papertitle><a href="https://aclanthology.org/2020.acl-main.214.pdf">Integrating Multimodal Information in Large Pretrained Transformers</a></papertitle><br>
                            Wasifur Rahman, <strong>Md Kamrul Hasan</strong>, Sangwu Lee, Amir Zadeh, Chengfeng Mao, Louis-Philippe Morency, and Ehsan Hoque. <br>
                            <b>ACL 2020</b> <br>
                            [<a href="https://aclanthology.org/2020.acl-main.214.pdf">PDF</a>]
                            [<a href="https://github.com/WasifurRahman/BERT_multimodal_transformer">Code</a>]<br>
                            <!-- [<a href="./bibtex/yang2021empirical.txt">Bibtex</a>] -->
                            <!-- [<a href="https://referit3d.github.io/benchmarks.html">Benchmarks</a>] -->
                            <grey> Can large pre-trained language model integrate non-verbal features? 
                        </td>
                    </tr>


                    <tr>
                        <!-- <tbody><tr bgcolor="#ffffd0"> -->
                        <td width="25%">
                            <a href="assets/img/urfunny1.png">
                            <img src="assets/img/urfunny1.png" width="230px" height="150px">
                        </td>
                        <td valign="top" width="75%">
                            <p>
                            <papertitle><a href="https://aclanthology.org/D19-1211.pdf">UR-FUNNY: A Multimodal Language Dataset for Understanding Humor</a></papertitle><br>
                            <strong>Md Kamrul Hasan</strong>, Wasifur Rahman, Amir Zadeh, Jianyuan Zhong, Md Iftekhar Tanveer,and Louis-Philippe Morency <br>
                            <b>EMNLP 2019</b> <br>
                            [<a href="https://aclanthology.org/D19-1211.pdf">PDF</a>]
                            [<a href="https://github.com/matalvepu/contextual-memory-fusion">Code</a>]
                            [<a href="https://github.com/ROC-HCI/UR-FUNNY">Data</a>]<br>
                            <grey> Proposed first multimodal dataset and baseline model C-MFN for humor detection.
                        </td>
                    </tr>

                    <tr>
                        <!-- <tbody><tr bgcolor="#ffffd0"> -->
                        <td width="25%">
                            <a href="assets/img/fbi.png">
                            <img src="assets/img/fbi.png" width="230px" height="150px">
                        </td>
                        <td valign="top" width="75%">
                            <p>
                            <papertitle><a href="https://ieeexplore.ieee.org/abstract/document/8925473">Facial Expression Based Imagination Index and a Transfer Learning Approach to Detect Deception</a></papertitle><br>
                            <strong>Md Kamrul Hasan</strong>, Wasifur Rahman, Luke Gerstner, Taylan Sen, Sangwu Lee, Kurtis Haut, and Ehsan Hoque<br>
                            <b>ACII 2019</b> <br>
                            [<a href="https://ieeexplore.ieee.org/abstract/document/8925473">PDF</a>]
                            [<a href="https://roc-hci.com/current-projects/deception-project/">Project</a>]<br>
                            <grey> Can baseline facial expression helps to detect deception during interview questions?.
                        </td>
                    </tr>

                    <tr>
                        <!-- <tbody><tr bgcolor="#ffffd0"> -->
                        <td width="25%">
                            <a href="assets/img/coherence.png">
                            <img src="assets/img/coherence.png" width="230px" height="150px">
                        </td>
                        <td valign="top" width="75%">
                            <p>
                            <papertitle><a href="">Unsupervised Text Segmentation using Coherence aware BERT</a></papertitle><br>
                            <strong>Md Kamrul Hasan</strong>, Md Mahmudul Hasan, and Faisal Ishtiaq<br>
                            <b>pre print</b> <br>
                            <grey> Designed unsupervised text segmentation alogrithm that achived SOTA performances in multiple datasets.
                        </td>
                    </tr>

                    </table>  


                    <!-- <div class="subheading mb-3">Preprint Papers</div>
                    <div style="color: black; font-size: 16px;">
                        <ol>
                            <li> <b>Md Kamrul Hasan</b>, et al.  “Unsupervised Text Segmentation using Coherence aware BERT", <b>EACL-2021</b>, (under-review)
                            </li>
                        </ol>
                    </div> -->

                    <div class="subheading mb-3">Other Papers</div>
                    <div style="color: black; font-size: 14px;">
                        <ol>
                          <!-- <li> <b>Md Kamrul Hasan</b>, Sangwu Lee, Wasifur Rahman, Amir Zadeh, Rada Mihalcea, Louis-Philippe Morency, and Ehsan Hoque. “Humor Knowledge Enriched Transformer for Understanding Multimodal Humor", <b style="color:#FF0000";>AAAI-2021(Accepted)</b>
                           </li>

                          <li>Wasifur Rahman, <b>Md Kamrul Hasan</b>, Sangwu Lee, Amir Zadeh, Chengfeng Mao, Louis-Philippe Morency, and Ehsan Hoque. “Integrating Multimodal Information in Large Pretrained Transformers", Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (<b>ACL</b>), 2020. <a href="https://www.aclweb.org/anthology/2020.acl-main.214/">[<u>PDF</u>]</a> <a href="https://github.com/matalvepu/multimodal-trasnformer">[<u>Code</u>]</a></li>

                          <li><b>Md Kamrul Hasan</b>, Wasifur Rahman, Amir Zadeh, Jianyuan Zhong, Md Iftekhar Tanveer,and Louis-Philippe Morency. “UR-FUNNY: A Multimodal Language Dataset for Understanding Humor.",Proceedings of the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (<b>EMNLP-IJCNLP</b>), 2019. <a href="https://www.aclweb.org/anthology/D19-1211/">[<u>PDF</u>]</a> <a href="https://github.com/matalvepu/contextual-memory-fusion">[<u>Code</u>]</a> <a href="https://github.com/ROC-HCI/UR-FUNNY">[<u>Data</u>]</a> </li> 

                          <li> <b>Md Kamrul Hasan</b>, et al. ``Facial Expression Based Imagination Index and a Transfer Learning Approach to Detect Deception",  8th International Conference on Affective Computing and Intelligent Interaction (<b>ACII</b>), 2019. <a href="https://roc-hci.com/wp-content/uploads/mentalface_ACII19.pdf">[<u>PDF</u>]</a> <a href="https://roc-hci.com/current-projects/deception-project/">[<u>Project</u>]</a> 
                          </li> -->
                          
                          <li> 
                            <b>Md Kamrul Hasan</b>, Taylan Sen, Yiming Yang, Raiyan Abdul Baten, Kurtis Glenn Haut, Mohammed Ehsan Hoque. "LIWC into the Eyes: Using Facial Features to Contextualize Linguistic Analysis in Multimodal Communication". In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII), pp. 1-7. IEEE, 2019. <a href="https://roc-hci.com/wp-content/uploads/LIWC_ACII19.pdf">[<u>PDF</u>]</a> <a href="https://roc-hci.com/current-projects/deception-project/">[<u>Project</u>]</a>
                          </li>

                          <li>
                          Taylan Sen, <b>Md Kamrul Hasan</b>, Minh Tran, Matt Levin, Yiming Yang, M. Ehsan Hoque.``Say CHEESE: the Common Habitual Expression Encoder for Smile Examination and its Application to Analyze Deceptive Communication''. Automatic Face and Gesture Recognition Conference (<b>FG</b>), 2018. <a href="https://roc-hci.com/wp-content/uploads/cheese.pdf">[<u>PDF</u>]</a> <a href="https://roc-hci.com/current-projects/deception-project/">[<u>Project</u>]</a>
                          </li>
                          
                          
                          <li>
                                Taylan Sen, <b>Md Kamrul Hasan</b>, Zach Teicher, Mohammed Ehsan Hoque. "Automated dyadic data recorder (ADDR) framework and analysis of facial cues in deceptive communication". Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 1, no. 4 (2018): 1-22. <a href="https://dl.acm.org/doi/pdf/10.1145/3161178?casa_token=GRVnG8DIc4MAAAAA:lH1CYoCUH0NNu3tCr4Ea2JQUjTwXDnklnZ2QD1saZ_mURJbMgQknDFPUL2eVCBOK-48B7S6rvEtZ">[<u>PDF</u>]</a>
                            </li>

                            <li>
                                Rasoul Shafipour, Raiyan Abdul Baten, <b>Md Kamrul Hasan</b>, Gourab Ghoshal, Gonzalo Mateos, Mohammed Ehsan Hoque. "Buildup of speaking skills in an online learning community: a network-analytic exploration".  Palgrave Communications, 4(1), 1-10. <a href="https://www.nature.com/articles/s41599-018-0116-6">[<u>PDF</u>]</a>
                            </li>
                        </ol>
                    </div>

                    <!-- <div class="subheading mb-3">Journal Papers</div>
                    <div style="color: black; font-size: 16px;">
                        <ol>
                            <li>
                                Taylan Sen, <b>Md Kamrul Hasan</b>, Zach Teicher, Mohammed Ehsan Hoque. "Automated dyadic data recorder (ADDR) framework and analysis of facial cues in deceptive communication". Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 1, no. 4 (2018): 1-22. <a href="https://dl.acm.org/doi/pdf/10.1145/3161178?casa_token=GRVnG8DIc4MAAAAA:lH1CYoCUH0NNu3tCr4Ea2JQUjTwXDnklnZ2QD1saZ_mURJbMgQknDFPUL2eVCBOK-48B7S6rvEtZ">[<u>PDF</u>]</a>
                            </li>

                            <li>
                                Rasoul Shafipour, Raiyan Abdul Baten, <b>Md Kamrul Hasan</b>, Gourab Ghoshal, Gonzalo Mateos, Mohammed Ehsan Hoque. "Buildup of speaking skills in an online learning community: a network-analytic exploration".  Palgrave Communications, 4(1), 1-10. <a href="https://www.nature.com/articles/s41599-018-0116-6">[<u>PDF</u>]</a>
                            </li>

                        </ol>
                    </div> -->
                </div>
            </section>
            <hr class="m-0" />


            <!-- reserach-->
            <section class="resume-section" id="reserach" style="font-size:16px">
                <div class="resume-section-content">
                    <h2 class="mb-5">Research Projects</h2>
<!--                     <div class="subheading mb-3"> <span class="text-primary">Multimodal Humor Understanding</span></div> -->


                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

                        <tbody>
                        <!-- <tbody><tr bgcolor="#ffffd0"> -->
                        

                        <tr><td valign="top" width="75%">
                            <p>
                            <span class="text-primary" style="font-size:20px;float:left;">Multimodal Representation Learning</span> </br></br>

                            Collecting large-scale multimodal behavioral video datasets (e.g humor, sentiment prediction etc.) is extremely expensive, both in terms of time and money. There are no pre-trained models for visual or acoustic modalities which are suitable for studying non-verbal cues of human behavior. This project aims to learn multimodal representation from million of video utterances in self-supervised manner. The pre-trained model will be helpful for the downstream multimodal behavioral tasks with small datasets. </br> 
                            Publications:  <a href="" style="color:blue;">Thesis (work in progress)</a>
                        </td>
                        <!-- <td width="25%">
                            <a href="assets/img/dbates.png">
                            <img src="assets/img/dbates.png" width="200px" height="150px">
                        </td> -->
                        </tr>


                        <tr><td valign="top" width="70%">
                            <p>
                            <span class="text-primary" style="font-size:20px;float:left;">Multimodal Humor Understanding</span> </br></br>
                            Humor produced in multimodal manner, through the usage of words (text), gestures (visual) and prosodic cues (acoustic). We introduced UR-FUNNY- the first video dataset for humor detection task. 8257 humorous punchlines are presented, along with the prior sentences that build up their respective contexts. Total duration of the dataset is 90 hours. UR-FUNNY opens the door to the research community for studying multimodal cues involved in expressing humor.</br> 
                            Publications:  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17534" style="color:blue; text-decoration: underline;">AAAI 2021</a> , <a href="https://www.aclweb.org/anthology/D19-1211/" style="color:blue; text-decoration: underline;">EMNLP 2019</a> 
                        </td>
                        <td width="30%">
                            <a href="assets/img/urfunny1.png">
                            <img src="assets/img/urfunny1.png" width="220px" height="150px">
                        </td>
                        </tr>

                        <tr><td valign="top" width="70%">
                            <p>
                            <span class="text-primary" style="font-size:20px;float:left;">Multimodal Sentiment Analysis</span> </br></br>
                            Predicting sentiment in video utterance using multimodal signal involving language,visual and acoustic modalities. Designing deep learning algorithms that can capture both intra-modality and inter-modality interactions among these signals. We experiment the models on two popular multimodal sentiment analysis datasets of CMU-MOSI and CMU-MOSEI.</br> 
                            Publications:  <a href="https://www.aclweb.org/anthology/2020.acl-main.214/" style="color:blue; text-decoration: underline;">ACL 2020</a> , AAAI 21 (under-review) 
                        </td>
                        <td width="30%">
                            <a href="assets/img/mft.png">
                            <img src="assets/img/mft.png" width="200px" height="150px">
                        </td>
                        </tr>

                        <tr><td valign="top" width="75%">
                            <p>
                            <span class="text-primary" style="font-size:20px;float:left;">Multimodal Argument Analysis</span> </br></br>
                            The current literature mostly considers textual content while assessing the quality of an argument, and is limited to datasets containing short sequences (18-48 words). In this project, we study argument quality assessment in a multimodal context, and experiment on DBATES, a publicly available dataset of long debate videos.</br> 
                            Publications:  <a href="" style="color:blue;">EMNLP 2021</a> , <a href="https://arxiv.org/abs/2103.14189" style="color:blue;text-decoration: underline;">Transcation on Affective Computing</a> 
                        </td>
                        <td width="25%">
                            <a href="assets/img/dbates_com.png">
                            <img src="assets/img/dbates_com.png" width="200px" height="150px">
                        </td>
                        </tr>


                        <tr><td valign="top" width="75%">
                            <p>
                            <span class="text-primary" style="font-size:20px;float:left;">Credibility Understanding</span> </br></br>
                            Is there a chance that a computer can aid a human in detecting deceptive behavior? Are there other facial features that could indicate deception? Could you combine linguistic characteristics to aid in lie detection? How is all this data being collected and analyzed in the first place? The follwoing papers in address these types of questions and raise even more interesting ones.</br> 
                            Publications: <a href="https://ieeexplore.ieee.org/abstract/document/8925473" style="color:blue; text-decoration: underline;">ACII 2019.a </a>, <a href="https://ieeexplore.ieee.org/document/8925467" style="color:blue; text-decoration: underline;">ACII 2019.b </a>, <a href="https://ieeexplore.ieee.org/document/8373851" style="color:blue; text-decoration: underline;">FG 2018 </a>, <a href="https://dl.acm.org/doi/abs/10.1145/3161178" style="color:blue; text-decoration: underline;">UBICOMP 2018 </a> </br>
                        </td>
                        <td width="25%">
                            <!-- <a href="assets/img/dbates.png">
                            <img src="assets/img/dbates.png" width="200px" height="150px">
 -->
                            
                            <iframe width="300" height="150" src="https://www.youtube.com/embed/Jfli-6Q-13Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

                            <!-- <iframe width="200" height="150" src="https://www.youtube.com/watch?v=Jfli-6Q-13Q&ab_channel=UniversityofRochester" frameborder="0" allowfullscreen>
                            </iframe> -->
                        </td>
                        </tr>


                    </table>
                    <!-- <div>

                        <span class="text-primary" style="font-size:20px;float:left;">Multimodal Humor Understanding</span> </br></br>
                        
                        <img src="assets/img/urfunny.png" style="width:35%;height:25%; float: right; " />

                        <p style="font-size:14px;width:65%;float:left;";>
                        
                        We introduced UR-FUNNY- the first video dataset for humor detection task. 8257 humorous punchlines are presented, along with the prior sentences that build up their respective contexts. Total duration of the dataset is 90 hours. This dataset has been collected from 1,866 videos featuring 1,741 TED Talks speakers using audience laughter signal. UR-FUNNY opens the door to the research community for studying multimodal cues involved in expressing humor.</br>
                        
                        Deeveloped Contextual Memory Fusion Network to detect humor from the video utterance. Using this model we ran experiments to understand the importance of each modalities as well as the background context. </br></br>

                        Publications:  <a href="https://www.aclweb.org/anthology/D19-1211/" style="color:blue; text-decoration: underline;">EMNLP-IJCNLP, 2019</a> </br>
                        </p> 
                        
                    </div>


                    </br></br>

                    <div class="justify-content-between" style="padding-top: 20px;padding-bottom: 20px; float : left;">

                        <span class="text-primary" style="font-size:20px; float:left;">Humor Knowledge Enriched Transformer</span>  <br><br>
                    
                        <p style="font-size:14px;width:55%;float:left;";>

                        Recognizing humor from a video  requires understanding the verbal and non-verbal components as well as incorporating the appropriate context and external knowledge. We have designed Humor Knowledge Enriched Transformer (HKT) that can capture the gist of a multimodal humorous expression by integrating the preceding context and external knowledge. We incorporate humor-centric external knowledge into the model by capturing the ambiguity and sentiment present in the language. We encode all the textual, acoustic, visual, and external knowledge separately using Transformer based Encoders, followed by a cross attention layer to exchange information among them.
                        </br> </br>
                        Publications: <a href="https://aaai.org/Conferences/AAAI-21/" style="color:blue; text-decoration: underline;">AAAI, 2021 (Accepted)</a> </br>
                        </p> 
                        <img src="assets/img/hkt_full2.jpg" style="width:45%;height:20%; float: right; " />
                    </div>

                    </br></br>


                    <div class="justify-content-between" style="padding-top: 20px;padding-bottom: 20px; float : left;">

                        <span class="text-primary" style="font-size:20px; float:left;">Multimodal Transformer for Sentiment Analysis</span>  <br><br>
                    
                        <p style="font-size:14px;width:65%;float:left;";>

                        Transformer based pretrained language models like BERT and XLNet do not have necessary components to accept other modalities like vision and acoustic. We designed Multimodal Adaptation Gate (MAG) that allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning. It does so by generating a shift to internal representation of BERT and XLNet; a shift that is conditioned on the visual and acoustic modalities. </br>
                       
                        We have achieved superior performance on both CMU-MOSI and CMU-MOSEI datasets (commonly used datasets for multimodal sentiment analysis). On the CMU-MOSI dataset, MAG-XLNet achieves human level multimodal sentiment analysis performance for the first time in the NLP community.  </br> </br>
                        Publications:<a href="https://www.aclweb.org/anthology/2020.acl-main.214/" style="color:blue; text-decoration: underline;">ACL, 2020</a>  </br>
                        </p> 
                        <img src="assets/img/mbert.png" style="width:22%;height:4%; float: right; " />
                    </div>
                    

                    </br></br>

                     <div class="justify-content-between" style="padding-top: 20px;padding-bottom: 20px; float : left;">

                        <span class="text-primary" style="font-size:20px; float:left;">Unsupervised Text Segmentation</span>  <br><br>
                   
                        <p style="font-size:14px;width:70%;float:left;";>

                        Breaking down long text into semantically coherent segments is very useful for downstream applications like summarization and information retrieval. We designed unsupervised text segmentation algorithm named Coherence aware BERT (C-BERT) Tiling that takes advantage of the pre-trained BERT language model to find topical shifts in a document. The next sentence prediction objective of BERT trained on billions of text is used to measure coherence between adjacent text segments. These locally attended coherence scores is utilized to identify the semantic discourses in a document of arbitrary length without learning dataset specific distribution. We establish a new state-of-the-art results in three standard datasets, thus facilitating real-world applications.
                        </br> </br>
                        Publications: <b >EACL, 2021 (Under Review)</b>  </br>
                        </p> 

                        <img src="assets/img/coherence.png" style="width:30%;height:20%; float: right; " />
                    </div>

                     </br></br>

                     <div class="justify-content-between" style="padding-top: 20px;padding-bottom: 20px; float : left;">

                        <span class="text-primary" style="font-size:20px; float:left;">Credibility Understanding</span>  <br><br>
                   
                        <p style="font-size:14px;width:65%;float:left;";>

                        The baseline facial expression pattern can help to understand credibility during real interrogation. In an interrogation game, the interrogator ask baseline questions before the main interrogation. First, we trained a LSTM to model baseline facial expression sequences. Using a <b>transfer learning</b> approach, the pretrained model from the baseline used to extract face embedding during the main interrogation and predict truth vs bluff.

                        </br> </br>
                        Publications: <a href="http://hoques.com/Publications/2019/2019-Mental-face-Hasan-etal-ACII.pdf" style="color:blue; text-decoration: underline;">ACII, 2019</a>  </br>
                        </p> 

                        <img src="assets/img/fbindex.png" style="width:35%;height:20%; float: right; " />
                    </div>

                </div> -->



            </section>
            <hr class="m-0" />
            <!-- Interests-->
            <section class="resume-section" id="resources">
                <div class="resume-section-content">

                    <div class="subheading mb-3">Datasets</div>
                    <div style="color: black; font-size: 16px;">
                        <ol>
                           <li> UR-FUNNY: Multimodal Dataset for Humor Understanding.  <a style="color:blue; text-decoration: underline;" href="https://github.com/ROC-HCI/UR-FUNNY" >[<u>Data</u>]</a>

                           </li>
                            <li> 
                                UR-LYING: Video Dataset for Credibility Understanding. <a style="color:blue; text-decoration: underline;" href="https://docs.google.com/forms/d/e/1FAIpQLSeYx2Kn5F4XQWCj8JF6aL1pTrxUaTcU_pYmP3GezRGBF1JI2Q/viewform">[<u>Data</u>]</a>
                            </li>
                        </ol>
                    </div>

                    <div class="subheading mb-3">Opensource</div>
                    <div style="color: black; font-size: 16px;">
                        <ol>
                            <li>  <a style="color:blue; text-decoration: underline;" href="https://github.com/matalvepu/HKT"> <u>Humor Knowledge Enriched Transformer</u></a>

                           </li>

                           <li>  <a style="color:blue; text-decoration: underline;" href="https://github.com/matalvepu/multimodal-trasnformer"> <u>Multimodal Transformer</u></a>

                           </li>

                            <li> 
                                <a style="color:blue; text-decoration: underline;" href="https://github.com/matalvepu/contextual-memory-fusion"> <u>Memory Fusion Network</u></a>
                            </li>

                            <li> 
                                <a style="color:blue; text-decoration: underline;" href="https://github.com/matalvepu/HMM-Decoder"> <u>HMM Decoder for part of Speech Tagging</u></a>
                            </li>

                            <li> 
                                <a style="color:blue; text-decoration: underline;" href="https://github.com/matalvepu/EM-for-GMM"> <u>EM fitting of a Mixture of Gaussians on the two dimensional points</u></a>
                            </li>

                        </ol>
                    </div>

                </div>
            </section>
            <hr class="m-0" />
            <!-- Awards-->
            <section class="resume-section" id="contact">
                <div class="resume-section-content">
                    <h2 class="mb-5">Contact</h2>
                    <p class="lead mb-5 " style="color: black; font-size: 16px;"> 
                    
                    Department of Computer Science </br>
                    2513 Wegmans Hall </br>
                    Box 270226 </br>
                    University of Rochester </br>
                    Rochester, NY 14627 </br>
                    
                    <b>Email: mhasan8@cs.rochester.edu </b>

                    </p>
                </div>
            </section>
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
