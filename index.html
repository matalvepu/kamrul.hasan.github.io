<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Resume - Start Bootstrap Theme</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.13.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <!-- <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" /> -->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Hasan Kamrul</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="" /></span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger"  href="#experience">Experience</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#publication">Publication</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#reserach">Reserach</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#resources">Resources</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#contact">Contact</a></li>
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <!-- <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Kamrul
                        <span class="text-primary">Hasan</span>
                    </h1>
                    <div class="subheading mb-5">
                       PhD Candidate @ University of Rochester
                    </div>
                        <p class="lead mb-5 " style="color: black; font-size: 14px;"> I am a PhD candidate at the Univcersity of Rochester, USA. I received my MS degree in Computer Science from the University of Rochester in 2018 and B.Sc. in Computer Science and Engineering from Bangladesh University of Engineering and Technology in 2014.
                        </br> 
                        My research focused on theoretical and empirical aspects of <b>multimodal machine learning</b>. It is concerned with joint modeling ofmultiple modalities (language, acoustic & visual) to understand human language as it happens face-to-face communications. My supervisor is <a href="https://www.aclweb.org/anthology/D19-1211/" style="color:blue; text-decoration: underline;">Ehsan Hoque</a>. 
                        </br></br> 
                        <a href="https://hoques.com/" style="color:blue; text-decoration: underline;">resume</a> 
                        </br></br> 
                        </p>
                    </div> 
                    </br></br> </br></br> </br></br> 
                    <div class="social-icons">
                        <a class="social-icon" href="https://www.linkedin.com/in/md-kamrul-hasan-06a02845/"><i class="fab fa-linkedin-in"></i></a>
                        <a class="social-icon" href="https://github.com/matalvepu"><i class="fab fa-github"></i></a>
                        <a class="social-icon" href="#"><i class="fab fa-twitter"></i></a>
                        <a class="social-icon" href="#"><i class="fab fa-google"></i></a>
                    </div>
                </div>
            </section> -->

            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Kamrul
                        <span class="text-primary">Hasan</span>
                    </h1>
                    <div class="subheading mb-5">
                       PhD Candidate @ University of Rochester
                    </div>
                        <p class="lead mb-5 " style="color: black; font-size: 16px;"> 
                            I am a PhD candidate at the Univcersity of Rochester, USA. I received my MS degree in Computer Science from the University of Rochester in 2018 and B.Sc. in Computer Science and Engineering from Bangladesh University of Engineering and Technology in 2014.
                            </br> </br> 
                            My research focused on theoretical and empirical aspects of <b>multimodal machine learning</b>. It is concerned with joint modeling of multiple modalities (language, acoustic & visual) to understand human language as it happens face-to-face communications. My reserach works spans in the following areas: humor, sentiment, credibility understanding, text segmentation etc. My supervisor is <a href="https://www.aclweb.org/anthology/D19-1211/" style="color:blue; text-decoration: underline;">Ehsan Hoque</a>. 
                            </br></br> 
                            <a href="https://drive.google.com/file/d/1CXrYmP1Hw9T6rlnUQZUXNODQfQ1E_nrl/view?usp=sharing" style="color:blue; text-decoration: underline; font-size: 20px;">Resume</a> 
                            </br> 
                        </p>
                    
                    <div class="social-icons">
                        <a class="social-icon" href="https://www.linkedin.com/in/md-kamrul-hasan-06a02845/" style="width:50px;height: 50px;"><i class="fab fa-linkedin-in"></i></a>
                        <a class="social-icon" style="width:50px;height: 50px;" href="https://github.com/matalvepu"><i class="fab fa-github"></i></a>
                        <a class="social-icon" style="width:50px;height: 50px;" href="https://scholar.google.com/citations?user=zLGFlIgAAAAJ&hl=en"><i class="fab fa-google"></i></a>

                       <!--  <a class="social-icon" style="width:50px;height: 50px;" href="https://scholar.google.com/citations?user=zLGFlIgAAAAJ&hl=en"><i class="ai ai-google-scholar"></i></a> -->

                    </div>
                </div>
            </section>

            <hr class="m-0" />
            <!-- Experience-->
            <section class="resume-section" id="experience">
                <div class="resume-section-content">
                    <h2 class="mb-5" style="margin-bottom: 1.5rem !important;">Experience</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5" style="margin-bottom: 1rem !important;">
                        <div class="flex-grow-1">
                            <h3 class="mb-2">Research Intern</h3>
                            <div class="subheading mb-3">Comcast AI</div>
                            <p>Developed unsupervised text segmentation algorithm. </p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">July 2020 - Sept 2020</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5" style="margin-bottom: 1rem !important;">
                        <div class="flex-grow-1">
                            <h3 class="mb-2">Applied Scientist Intern</h3>
                            <div class="subheading mb-3">Amazon</div>
                            <p>Developed alogortihm for multimodal emotion recognition.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">May 2019 - Aug 2019</span></div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between" style="margin-bottom: 1rem !important;">
                        <div class="flex-grow-1">
                            <h3 class="mb-2">Software Engineer</h3>
                            <div class="subheading mb-3">Therap Services LLC</div>
                            <p>.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">Aug 2014 - May 2015</span></div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Education-->
            <section class="resume-section" id="publication">
                <div class="resume-section-content">
                    <h2 class="mb-5">Publication</h2>

                    <div class="subheading mb-3">Preprint Papers</div>
                    <div style="color: black; font-size: 16px;">
                        <ol>
                           <li> <b>Md Kamrul Hasan</b>, Sangwu Lee, Wasifur Rahman, Amir Zadeh, Rada Mihalcea, Louis-Philippe Morency, and Ehsan Hoque. “Humor Knowledge Enriched Transformer for Understanding Multimodal Humor", <b>AAAI-2021</b>, (under-review)
                           </li>
                            <li> <b>Md Kamrul Hasan</b>, et al.  “Unsupervised Text Segmentation using Coherence aware BERT", <b>EACL-2021</b>, (under-review)
                            </li>
                        </ol>
                    </div>

                    <div class="subheading mb-3">Conferece Papers</div>
                    <div style="color: black; font-size: 16px;">
                        <ol>
                          <li>Wasifur Rahman, <b>Md Kamrul Hasan</b> , Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe Morency, and EhsanHoque. “Integrating Multimodal Information in Large Pretrained Transformers", Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (<b>ACL</b>), 2020. <a href="https://www.aclweb.org/anthology/2020.acl-main.214/">[<u>PDF</u>]</a> <a href="https://github.com/matalvepu/multimodal-trasnformer">[<u>Code</u>]</a></li>

                          <li><b>Md Kamrul Hasan</b> , WasifurRahman,AmirZadeh,JianyuanZhong,MdIftekharTanveer,andLouis-PhilippeMorency. “UR-FUNNY: A Multimodal Language Dataset for Understanding Humor.",Proceedings of the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (<b>EMNLP-IJCNLP</b>), 2019. <a href="https://www.aclweb.org/anthology/D19-1211/">[<u>PDF</u>]</a> <a href="https://github.com/matalvepu/contextual-memory-fusion">[<u>Code</u>]</a> <a href="https://github.com/ROC-HCI/UR-FUNNY">[<u>Data</u>]</a> </li> 

                          <li> <b>Md Kamrul Hasan</b>, et al. ``Facial Expression Based Imagination Index and a Transfer Learning Approach to Detect Deception",  8th International Conference on Affective Computing and Intelligent Interaction (<b>ACII</b>), 2019. <a href="https://roc-hci.com/wp-content/uploads/mentalface_ACII19.pdf">[<u>PDF</u>]</a> <a href="https://roc-hci.com/current-projects/deception-project/">[<u>Project</u>]</a> 
                          </li>
                          
                          <li>
                          Taylan Sen, <b>Md Kamrul Hasan</b>, Minh Tran, Matt Levin, Yiming Yang, M. Ehsan Hoque.``Say CHEESE: the Common Habitual Expression Encoder for Smile Examination and its Application to Analyze Deceptive Communication''. Automatic Face and Gesture Recognition Conference (<b>FG</b>), 2018. <a href="https://roc-hci.com/wp-content/uploads/cheese.pdf">[<u>PDF</u>]</a> <a href="https://roc-hci.com/current-projects/deception-project/">[<u>Project</u>]</a>
                          </li>
                          
                          <li> 
                            <b>Md Kamrul Hasan</b>, Taylan Sen, Yiming Yang, Raiyan Abdul Baten, Kurtis Glenn Haut, Mohammed Ehsan Hoque. "LIWC into the Eyes: Using Facial Features to Contextualize Linguistic Analysis in Multimodal Communication". In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII), pp. 1-7. IEEE, 2019. <a href="https://roc-hci.com/wp-content/uploads/LIWC_ACII19.pdf">[<u>PDF</u>]</a> <a href="https://roc-hci.com/current-projects/deception-project/">[<u>Project</u>]</a>
                          </li>

                        </ol>
                    </div>

                    <div class="subheading mb-3">Journal Papers</div>
                    <div style="color: black; font-size: 16px;">
                        <ol>
                            <li>
                                Taylan Sen, <b>Md Kamrul Hasan</b>, Zach Teicher, Mohammed Ehsan Hoque. "Automated dyadic data recorder (ADDR) framework and analysis of facial cues in deceptive communication". Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 1, no. 4 (2018): 1-22. <a href="https://dl.acm.org/doi/pdf/10.1145/3161178?casa_token=GRVnG8DIc4MAAAAA:lH1CYoCUH0NNu3tCr4Ea2JQUjTwXDnklnZ2QD1saZ_mURJbMgQknDFPUL2eVCBOK-48B7S6rvEtZ">[<u>PDF</u>]</a>
                            </li>

                            <li>
                                Rasoul Shafipour, Raiyan Abdul Baten, <b>Md Kamrul Hasan</b>, Gourab Ghoshal, Gonzalo Mateos, Mohammed Ehsan Hoque. "Buildup of speaking skills in an online learning community: a network-analytic exploration".  Palgrave Communications, 4(1), 1-10. <a href="https://www.nature.com/articles/s41599-018-0116-6">[<u>PDF</u>]</a>
                            </li>

                        </ol>
                    </div>
                </div>
            </section>
            <hr class="m-0" />


            <!-- reserach-->
            <section class="resume-section" id="reserach">
                <div class="resume-section-content">
                    <h2 class="mb-5">Research Projects</h2>
<!--                     <div class="subheading mb-3"> <span class="text-primary">Multimodal Humor Understanding</span></div> -->
                    <div>

                        <span class="text-primary" style="font-size:20px;float:left;">Multimodal Humor Understanding</span> </br></br>
                        
                        <img src="assets/img/urfunny.png" style="width:35%;height:25%; float: right; " /><p style="font-size:14px;width:65%;float:left;";>
                        
                        We introduced UR-FUNNY- the first video dataset for humor detection task. 8257 humorous punchlines are presented, along with the prior sentences that build up their respective contexts. Total duration of the dataset is 90 hours. This dataset has been collected from 1,866 videos featuring 1,741 TED Talks speakers using audience laughter signal. UR-FUNNY opens the door to the research community for studying multimodal cues involved in expressing humor.</br>
                        
                        Deeveloped Contextual Memory Fusion Network to detect humor from the video utterance. Using this model we ran experiments to understand the importance of each modalities as well as the background context. </br></br>

                        Publications:  <a href="https://www.aclweb.org/anthology/D19-1211/" style="color:blue; text-decoration: underline;">EMNLP-IJCNLP, 2019</a> </br>
                        </p> 
                        
                    </div>

                    </br></br>


                    <div class="justify-content-between" style="padding-top: 20px;padding-bottom: 20px; float : left;">

                        <span class="text-primary" style="font-size:20px; float:left;">Multimodal Transformer for Sentiment Analysis</span>  <br><br>
                    
                        <p style="font-size:14px;width:70%;float:left;";>

                        Transformer based pretrained language models like BERT and XLNet do not have necessary components to accept other modalities like vision and acoustic. We designed Multimodal Adaptation Gate (MAG) that allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning. It does so by generating a shift to internal representation of BERT and XLNet; a shift that is conditioned on the visual and acoustic modalities. </br>
                       
                        We have achieved superior performance on both CMU-MOSI and CMU-MOSEI datasets (commonly used datasets for multimodal sentiment analysis). On the CMU-MOSI dataset, MAG-XLNet achieves human level multimodal sentiment analysis performance for the first time in the NLP community.  </br> </br>
                        Publications:<a href="https://www.aclweb.org/anthology/2020.acl-main.214/" style="color:blue; text-decoration: underline;">ACL, 2020</a>  </br>
                        </p> 
                        <img src="assets/img/mbert.png" style="width:23%;height:10%; float: right; " />
                    </div>
                    

                    </br></br>

                    <div class="justify-content-between" style="padding-top: 20px;padding-bottom: 20px; float : left;">

                        <span class="text-primary" style="font-size:20px; float:left;">Humor Knowledge Enriched Transformer</span>  <br><br>
                    
                        <p style="font-size:14px;width:70%;float:left;";>

                        Recognizing humor from a video  requires understanding the verbal and non-verbal components as well as incorporating the appropriate context and external knowledge. We have designed Humor Knowledge Enriched Transformer (HKT) that can capture the gist of a multimodal humorous expression by integrating the preceding context and external knowledge. We incorporate humor-centric external knowledge into the model by capturing the ambiguity and sentiment present in the language. We encode all the textual, acoustic, visual, and external knowledge separately using Transformer based Encoders, followed by a cross attention layer to exchange information among them.
                        </br> </br>
                        Publications: <b >AAAI, 2021 (Under Review)</b>  </br>
                        </p> 
                        <img src="assets/img/hkt.png" style="width:28%;height:10%; float: right; " />
                    </div>
                    </br></br>

                     <div class="justify-content-between" style="padding-top: 20px;padding-bottom: 20px; float : left;">

                        <span class="text-primary" style="font-size:20px; float:left;">Unsupervised Text Segmentation</span>  <br><br>
                   
                        <p style="font-size:14px;width:70%;float:left;";>

                        Breaking down long text into semantically coherent segments is very useful for downstream applications like summarization and information retrieval. We designed unsupervised text segmentation algorithm named Coherence aware BERT (C-BERT) Tiling that takes advantage of the pre-trained BERT language model to find topical shifts in a document. The next sentence prediction objective of BERT trained on billions of text is used to measure coherence between adjacent text segments. These locally attended coherence scores is utilized to identify the semantic discourses in a document of arbitrary length without learning dataset specific distribution. We establish a new state-of-the-art results in three standard datasets, thus facilitating real-world applications.
                        </br> </br>
                        Publications: <b >EACL, 2021 (Under Review)</b>  </br>
                        </p> 

                        <img src="assets/img/coherence.png" style="width:30%;height:20%; float: right; " />
                    </div>

                     </br></br>

                     <div class="justify-content-between" style="padding-top: 20px;padding-bottom: 20px; float : left;">

                        <span class="text-primary" style="font-size:20px; float:left;">Credibility Understanding</span>  <br><br>
                   
                        <p style="font-size:14px;width:65%;float:left;";>

                        The baseline facial expression pattern can help to understand credibility during real interrogation. In an interrogation game, the interrogator ask baseline questions before the main interrogation. First, we trained a LSTM to model baseline facial expression sequences. Using a <b>transfer learning</b> approach, the pretrained model from the baseline used to extract face embedding during the main interrogation and predict truth vs bluff.

                        </br> </br>
                        Publications: <a href="http://hoques.com/Publications/2019/2019-Mental-face-Hasan-etal-ACII.pdf" style="color:blue; text-decoration: underline;">ACII, 2019</a>  </br>
                        </p> 

                        <img src="assets/img/fbindex.png" style="width:35%;height:20%; float: right; " />
                    </div>

                </div>



            </section>
            <hr class="m-0" />
            <!-- Interests-->
            <section class="resume-section" id="resources">
                <div class="resume-section-content">

                    <div class="subheading mb-3">Datasets</div>
                    <div style="color: black; font-size: 16px;">
                        <ol>
                           <li> UR-FUNNY: Multimodal Dataset for Humor Understanding.  <a style="color:blue; text-decoration: underline;" href="https://github.com/ROC-HCI/UR-FUNNY" >[<u>Data</u>]</a>

                           </li>
                            <li> 
                                UR-LYING: Video Dataset for Credibility Understanding. <a style="color:blue; text-decoration: underline;" href="https://docs.google.com/forms/d/e/1FAIpQLSeYx2Kn5F4XQWCj8JF6aL1pTrxUaTcU_pYmP3GezRGBF1JI2Q/viewform">[<u>Data</u>]</a>
                            </li>
                        </ol>
                    </div>

                    <div class="subheading mb-3">Opensource</div>
                    <div style="color: black; font-size: 16px;">
                        <ol>
                           <li>  <a style="color:blue; text-decoration: underline;" href="https://github.com/matalvepu/multimodal-trasnformer"> <u>Multimodal Transformer</u></a>

                           </li>

                            <li> 
                                <a style="color:blue; text-decoration: underline;" href="https://github.com/matalvepu/contextual-memory-fusion"> <u>Memory Fusion Network</u></a>
                            </li>

                            <li> 
                                <a style="color:blue; text-decoration: underline;" href="https://github.com/matalvepu/HMM-Decoder"> <u>HMM Decoder for part of Speech Tagging</u></a>
                            </li>

                            <li> 
                                <a style="color:blue; text-decoration: underline;" href="https://github.com/matalvepu/EM-for-GMM"> <u>EM fitting of a Mixture of Gaussians on the two dimensional points</u></a>
                            </li>

                        </ol>
                    </div>

                </div>
            </section>
            <hr class="m-0" />
            <!-- Awards-->
            <section class="resume-section" id="contact">
                <div class="resume-section-content">
                    <h2 class="mb-5">Contact</h2>
                    <p class="lead mb-5 " style="color: black; font-size: 16px;"> 
                    
                    Department of Computer Science </br>
                    2513 Wegmans Hall </br>
                    Box 270226 </br>
                    University of Rochester </br>
                    Rochester, NY 14627 </br>
                    
                    <b>Email: mhasan8@cs.rochester.edu </b>

                    </p>
                </div>
            </section>
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
