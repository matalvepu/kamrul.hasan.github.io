<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Resume - Start Bootstrap Theme</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.13.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <!-- <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" /> -->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Hasan Kamrul</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="" /></span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger"  href="#experience">Experience</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#publication">Publication</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#reserach">Reserach</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#interests">Interests</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#awards">Awards</a></li>
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Kamrul
                        <span class="text-primary">Hasan</span>
                    </h1>
                    <div class="subheading mb-5">
                       PhD Candidate @ University of Rochesters
                    </div>
                    <p class="lead mb-5 " style="color: black">My research focused on theoretical and empirical aspects of multimodal machine learning. It is concerned with joint modeling ofmultiple modalities (language, acoustic & visual) to understand human language as it happens face-to-face communications.</p>
                    <div class="social-icons">
                        <a class="social-icon" href="https://www.linkedin.com/in/md-kamrul-hasan-06a02845/"><i class="fab fa-linkedin-in"></i></a>
                        <a class="social-icon" href="https://github.com/matalvepu"><i class="fab fa-github"></i></a>
                        <a class="social-icon" href="#"><i class="fab fa-twitter"></i></a>
                        <a class="social-icon" href="#"><i class="fab fa-facebook-f"></i></a>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Experience-->
            <section class="resume-section" id="experience">
                <div class="resume-section-content">
                    <h2 class="mb-5">Experience</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Research Intern</h3>
                            <div class="subheading mb-3">Comcast AI</div>
                            <p>I developed unsupervised text segmentation algorithm. </p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">July 2020 - Sept 2020</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Applied Scientist Intern</h3>
                            <div class="subheading mb-3">Amazon</div>
                            <p>Developed alogortihm for multimodal emotion recognition.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">May 2019 - Aug 2019</span></div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Software Engineer</h3>
                            <div class="subheading mb-3">Therap Services LLC</div>
                            <p>.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">Aug 2014 - May 2015</span></div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Education-->
            <section class="resume-section" id="publication">
                <div class="resume-section-content">
                    <h2 class="mb-5">Publication</h2>

                    <div class="subheading mb-3">Preprint Papers</div>
                    <div >
                        <ol>
                           <li> <b>Md Kamrul Hasan</b>, et al. “Humor Knowledge Enriched Transformer for Understanding Multimodal Humor", <b>AAAI-2021</b>, (under-review)
                           </li>
                            <li> <b>Md Kamrul Hasan</b>, et al.  “Unsupervised Text Segmentation using Coherence aware BERT", <b>EACL-2021</b>, (under-review)
                            </li>
                        </ol>
                    </div>

                    <div class="subheading mb-3">Conferece Papers</div>
                    <div >
                        <ol>
                          <li>Wasifur Rahman, <b>Md Kamrul Hasan</b> , et al. “Integrating Multimodal Information in Large Pretrained Transformers", Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (<b>ACL</b>), 2020. <a href="https://www.aclweb.org/anthology/2020.acl-main.214/">[<u>PDF</u>]</a> <a href="https://github.com/matalvepu/multimodal-trasnformer">[<u>Code</u>]</a></li>

                          <li><b>Md Kamrul Hasan</b> , et al. “UR-FUNNY: A Multimodal Language Dataset for Understanding Humor.",Proceedings of the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (<b>EMNLP-IJCNLP</b>), 2019. <a href="https://www.aclweb.org/anthology/D19-1211/">[<u>PDF</u>]</a> <a href="https://github.com/matalvepu/contextual-memory-fusion">[<u>Code</u>]</a> <a href="https://github.com/ROC-HCI/UR-FUNNY">[<u>Data</u>]</a> </li> 

                          <li> <b>Md Kamrul Hasan</b>, et al. ``Facial Expression Based Imagination Index and a Transfer Learning Approach to Detect Deception",  8th International Conference on Affective Computing and Intelligent Interaction (<b>ACII</b>), 2019. <a href="https://roc-hci.com/wp-content/uploads/mentalface_ACII19.pdf">[<u>PDF</u>]</a> <a href="https://roc-hci.com/current-projects/deception-project/">[<u>Project</u>]</a> 
                          </li>
                          
                          <li>
                          Taylan Sen, <b>Md Kamrul Hasan</b>, Minh Tran, Matt Levin, Yiming Yang, M. Ehsan Hoque.``Say CHEESE: the Common Habitual Expression Encoder for Smile Examination and its Application to Analyze Deceptive Communication''. Automatic Face and Gesture Recognition Conference (<b>FG</b>), 2018. <a href="https://roc-hci.com/wp-content/uploads/cheese.pdf">[<u>PDF</u>]</a> <a href="https://roc-hci.com/current-projects/deception-project/">[<u>Project</u>]</a>
                          </li>
                          
                          <li> 
                            <b>Md Kamrul Hasan</b>, Taylan Sen, Yiming Yang, Raiyan Abdul Baten, Kurtis Glenn Haut, Mohammed Ehsan Hoque. "LIWC into the Eyes: Using Facial Features to Contextualize Linguistic Analysis in Multimodal Communication". In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII), pp. 1-7. IEEE, 2019. <a href="https://roc-hci.com/wp-content/uploads/LIWC_ACII19.pdf">[<u>PDF</u>]</a> <a href="https://roc-hci.com/current-projects/deception-project/">[<u>Project</u>]</a>
                          </li>

                        </ol>
                    </div>

                    <div class="subheading mb-3">Journal Papers</div>
                    <div >
                        <ol>
                            <li>
                                Taylan Sen, <b>Md Kamrul Hasan</b>, Zach Teicher, Mohammed Ehsan Hoque. "Automated dyadic data recorder (ADDR) framework and analysis of facial cues in deceptive communication". Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 1, no. 4 (2018): 1-22. <a href="https://dl.acm.org/doi/pdf/10.1145/3161178?casa_token=GRVnG8DIc4MAAAAA:lH1CYoCUH0NNu3tCr4Ea2JQUjTwXDnklnZ2QD1saZ_mURJbMgQknDFPUL2eVCBOK-48B7S6rvEtZ">[<u>PDF</u>]</a>
                            </li>

                            <li>
                                Rasoul Shafipour, Raiyan Abdul Baten, <b>Md Kamrul Hasan</b>, Gourab Ghoshal, Gonzalo Mateos, Mohammed Ehsan Hoque. "Buildup of speaking skills in an online learning community: a network-analytic exploration".  Palgrave Communications, 4(1), 1-10. <a href="https://www.nature.com/articles/s41599-018-0116-6">[<u>PDF</u>]</a>
                            </li>

                        </ol>
                    </div>
                </div>
            </section>
            <hr class="m-0" />


            <!-- reserach-->
            <section class="resume-section" id="reserach">
                <div class="resume-section-content">
                    <h2 class="mb-5">Research Projects</h2>
                    <div class="subheading mb-3"> <span class="text-primary">Multimodal Humor Understanding</span></div>
                    <div>
                        <p style="font-size:14px;width:65%;float:left;";>
                        
                        Recognizing humor from a video requires understanding the verbal and non-verbal components as well as incorporating the appropriate background context. We have introduced UR-FUNNY- the first video dataset for humor detection task. 8257 humorous punchlines are presented, along with the prior sentences that build up their respective contexts. As negative data samples, 8257 non-humourous excerpts are also presented, where the last sentences are not followed by laughters. Total duration of the dataset is 90 hours. Importantly, unlike previous datasets where negative samples were drawn from different domains, UR-FUNNY draws the negative samples from the same videos as the humorous ones. This dataset has been collected from 1,866 videos featuring 1,741 TED Talks speakers using audience laughter signal. UR-FUNNY opens the door to the research community for studying multimodal cues involved in expressing humor.</br>
                        
                        We have developed a deep learning model to detect humor from the video utterance. The model is given a sequence of sentences (background context + punchline) along with vision and acoustic modalities. The goal is to detect whether the sequence will trigger immediate laughter after the punchline. The model has three components. The Unimodal Context Network has three LSTM (Long Short Term Memory) for capturing the unimodal summary of language, acoustic and vision used in background context. Then, Multimodal Context Summary use self attention mechanism to capture the interactions across these three modalities and create multimodal summary of background context. The last component is Memory Fusion Network that takes the language, acoustic and vision features used in punchline and try to predict the if the punchline is funny or not. During modeling punchline, it also takes input the unimodal and multimodal summary of context from the previous two component. That is the way it makes sure that punchline is model in light of context story. </br>

                        Using this model we ran experiments to understand the importance of each modalities as well as the background context. We have found that all of these components are crucial for understanding the punchline of a joke.
                        </p> 
                        <img src="assets/img/urfunny.png" style="width:35%;height:25%; float: right; " />
                    </div>

                    <div class="subheading mb-3"> <span class="text-primary">Multimodal Sentiment Analysis</span></div>
                    <div>
                        <p style="font-size:14px;width:65%;float:left;";>
                        
                        Human face-to-face communication is a complex multimodal signal. We use words (language modality), gestures (vision modality) and changes in tone (acoustic modality) to convey our intentions. Humans easily process and understand the emotion involved in face-to-face communication, however, comprehending this form of communication remains a significant challenge for Artificial Intelligence (AI). AI must understand each modality and the interactions between them that shape the contextual interpretation of our intention and emotions. </br>
                        Recent Transformer based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP. Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream. However, these pretrained models only work with language and do not have necessary component to accept other modalities like vision and acoustic.We designed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG). MAG allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning. It does so by generating a shift to internal representation of BERT and XLNet; a shift that is conditioned on the visual and acoustic modalities. During fine-tuning, this shifted vector modifies the internal state of the BERT and XLNet, allowing the models to seamlessly adapt to the multimodal input. </br>
                        In our experiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for multimodal sentiment analysis. Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the sentiment analysis performance. We have achieved superior performance on both datasets compare to the all baselines methods and establish new state or the art baseline. On the CMU-MOSI dataset, MAG-XLNet achieves human level multimodal sentiment analysis performance for the first time in the NLP community.
                                                </p> 
                        <img src="assets/img/mbert.png" style="width:35%;height:30%; float: right; " />
                    </div>

                </div>
            </section>
            <hr class="m-0" />
            <!-- Interests-->
            <section class="resume-section" id="interests">
                <div class="resume-section-content">
                    <h2 class="mb-5">Interests</h2>
                    <p>Apart from being a web developer, I enjoy most of my time being outdoors. In the winter, I am an avid skier and novice ice climber. During the warmer months here in Colorado, I enjoy mountain biking, free climbing, and kayaking.</p>
                    <p class="mb-0">When forced indoors, I follow a number of sci-fi and fantasy genre movies and television shows, I am an aspiring chef, and I spend a large amount of my free time exploring the latest technology advancements in the front-end web development world.</p>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Awards-->
            <section class="resume-section" id="awards">
                <div class="resume-section-content">
                    <h2 class="mb-5">Awards & Certifications</h2>
                    <ul class="fa-ul mb-0">
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Google Analytics Certified Developer
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Mobile Web Specialist - Google Certification
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>st</sup>
                            Place - University of Colorado Boulder - Emerging Tech Competition 2009
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>st</sup>
                            Place - University of Colorado Boulder - Adobe Creative Jam 2008 (UI Design Category)
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            2
                            <sup>nd</sup>
                            Place - University of Colorado Boulder - Emerging Tech Competition 2008
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>st</sup>
                            Place - James Buchanan High School - Hackathon 2006
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            3
                            <sup>rd</sup>
                            Place - James Buchanan High School - Hackathon 2005
                        </li>
                    </ul>
                </div>
            </section>
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
